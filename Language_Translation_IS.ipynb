{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "#Our Data is a text file of English-German sentence pairs.First we will read the file using the function defined below.\n",
    "\n",
    " # function to read raw text file\n",
    "def read_text(filename):\n",
    "  # open the file\n",
    "  file=open(filename, mode='rt', encoding='utf-8')\n",
    "  text=file.read()\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "\n",
    "\n",
    "#Now lets define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into english sentence and german sentences\n",
    "\n",
    "def to_lines(text):\n",
    "  sents=text.strip().split('\\n')\n",
    "  sents=[i.split('\\t') for i in sents]\n",
    "  return sents\n",
    "\n",
    "data = read_text(\"/content/deu.txt\")\n",
    "deu_eng=to_lines(data)\n",
    "deu_eng=array(deu_eng)\n",
    "\n",
    "The actual data contains over 150,000 sentence-pairs. However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. We can change this number as per our system computation power\n",
    "\n",
    "deu_eng=deu_eng[:50000, :]\n",
    "\n",
    "\n",
    "#Lets take a look at our data \n",
    "deu_eng\n",
    "\n",
    "array([['Go.',\t'Geh.','\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
    "       ['Hi.',\t'Hallo!',\t'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
    "      ['Hi.','\tGrÃ¼ÃŸ Gott!','\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)]',\n",
    "      [ 'Run!',\t'Lauf!',\t'CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)]',\n",
    "      ['Wow!',\t'Potzdonner!',\t'CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #2122382 (Pfirsichbaeumchen)]',]dtype='<U537')\n",
    "\n",
    "\n",
    "\n",
    "#empty Lists\n",
    "eng_l=[]\n",
    "deu_l=[]\n",
    "\n",
    "#populate the lists with sentence lengths\n",
    "for i in deu_eng[:,0]:\n",
    "  eng_l.append(len(i.split()))\n",
    "\n",
    "for i in deu_eng[:,1]:\n",
    "  eng_l.append(len(i.split()))\n",
    "\n",
    "length_df= pd.DataFrame({'eng':eng_l, 'deu':deu_l})\n",
    "\n",
    "length_df.hist(bins=30)\n",
    "plt.show()\n",
    "\n",
    "def tokenization(lines):\n",
    "  tokenizer=Tokenizer()\n",
    "  tokenizer.fit)on_texts(lines)\n",
    "  return tokenizer\n",
    "\n",
    "eng_tokenizer=tokenization(deu_eng[:, 0])\n",
    "eng_vocab_size=len(eng_tokenizer.word_index) +1\n",
    "\n",
    "eng_length=8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "\n",
    "\n",
    "deu_tokenizer=tokenization(deu_eng[:,1])\n",
    "deu_vocab_size=len(deu_tokenizer.word_index) +1\n",
    "\n",
    "deu_length=8\n",
    "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "  seq=tokenizer.texts_to_sequences(lines)\n",
    "  seq=pad_sequences(seq, maxlen=length, padding='post')\n",
    "  return seq\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test= train_test_split(deu_eng, test_size=0.2, random_state= 12)\n",
    "\n",
    "\n",
    "\n",
    "trainX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "\n",
    "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "       \n",
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "       model = Sequential()\n",
    "       model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "       model.add(LSTM(units))\n",
    "       model.add(RepeatVector(out_timesteps))\n",
    "       model.add(LSTM(units, return_sequences=True))\n",
    "       model.add(Dense(out_vocab, activation='softmax'))\n",
    "       return model\n",
    "\n",
    "model = bui;d_model(deu_vocab_size, eng_vocab_size, deu_length, 512)\n",
    "rms = optimizers.RMSprop(lr=0.001) \n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    " \n",
    "filename = 'model.h1.24_Homur'\n",
    "checkpoint = ModelCheckpoint(filename, monitor= 'val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "\n",
    "histroy = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1],1),\n",
    "                   epochs = 5, batch_size=512,\n",
    "                   validation_split = 0.2,\n",
    "                   callbacks=[checkpoint], verbose=1) \n",
    "plt.plot(histroy.histroy['loss'])\n",
    "plt.plot(histroy.histroy['val_loss'])  \n",
    "plt.legend(['train' , 'validation'])   \n",
    "plt.show()   \n",
    "       \n",
    "model = load_model('model.h1.24_Homur')       \n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0], testX.shape[1])))\n",
    "\n",
    "def get_word(n , tokenizer):\n",
    "       for word, index in tokenizer.word_index.items():\n",
    "           if index == n:\n",
    "               return word\n",
    "       return None\n",
    " \n",
    "preds_text = []\n",
    "for i in preds:\n",
    "       temp=[]\n",
    "       for j in range(len(i)):\n",
    "           t=get_word(i[j], eng_tokenizer)\n",
    "           if j>0:\n",
    "                if(t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                       temp.append('')\n",
    "                else:\n",
    "                       temp.append(t)\n",
    "               else:\n",
    "                       if(t== None):\n",
    "                           temp.append('')\n",
    "                       else:\n",
    "                            temp.append(t)\n",
    "               preds_text.append(''.join(temp))\n",
    "pred_df = pd.DataFrame({'actual': test[:,0], 'predicted' : preds_text}) \n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pred_df.head(15)       \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
